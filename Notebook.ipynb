{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review ML\n",
    "\n",
    "## Importações \n",
    "\n",
    "### Bibliotecas para cálculos com array \n",
    "Pacotes de biblioteca cientificas para calculos com arrays n-dimensionais e outras funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos\n",
    "\n",
    "Modelos de classificação linear, Naive Bayes e SVN como soluções mais utilizadas nas pesquisas que são simples e que atingem o objetivo de lidar com textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engenharia de características\n",
    "\n",
    "Foram utilizadas as bibliotecas TextBlob e Tfidf, sendo essa propria da Scikit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dense_transform import DenseTransformer, tokenizer, tokenizer_correct\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,  CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos\n",
    "\n",
    "Para a geração de gráficos foi utilizado o Matplot lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação de Modelo\n",
    "\n",
    "Foi utilizada a biblioteca sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Processamento dos dados utilizando filtragem pelos metodos: gaussian, bernoulli, multinomial, Linear e SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time\n",
    "\n",
    "Biblioteca para acesso ao tempo e conversões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenção de Data\n",
    "\n",
    "Método utilizado para a obtenção dos dados que irão ser tratados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data():\n",
    "    t0 = time.time()\n",
    "    global paths_df\n",
    "    # Load dirs name\n",
    "    cur_dir = os.path.realpath('.')\n",
    "    pos_dir = os.path.join(cur_dir, 'train-pos')\n",
    "    neg_dir = os.path.join(cur_dir, 'train-neg')\n",
    "\n",
    "\n",
    "    # Load files names\n",
    "    list_pos_dir = [ (os.path.join(pos_dir, x), 1) for x in os.listdir(pos_dir)][:11]\n",
    "    list_neg_dir = [ (os.path.join(neg_dir, x), 0) for x in os.listdir(neg_dir)][:11]\n",
    "    print(\"registers: {}\".format(len(list_pos_dir+list_neg_dir)))\n",
    "    print(\"Attention with 6000 registers it will consume about 5+GB of ram\")\n",
    "    # input(\"Continue? or press CTRL+C\")\n",
    "\n",
    "    # Mount data with label data frame\n",
    "    paths_df = pandas.DataFrame(list_pos_dir+list_neg_dir, columns=['path', 'label'])\n",
    "\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to obtain data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar números de tokens\n",
    "\n",
    "Metodo utilizado para a obtenção do número de tokens da aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_number_of_tokens():\n",
    "    t0 = time.time()\n",
    "    # Verify difference between size of tokens with tokenizer stem, stopwords\n",
    "    tfidf_stem = TfidfVectorizer(input='filename', stop_words='english', tokenizer=tokenizer)\n",
    "    tfidf_tokenizer_correct = TfidfVectorizer(input='filename', stop_words='english', tokenizer=tokenizer_correct)\n",
    "    tfidf_stem.fit(paths_df.path.values)\n",
    "    tfidf_stem.get_feature_names()\n",
    "    tfidf_stop = TfidfVectorizer(input='filename', stop_words='english')\n",
    "    tfidf_word = TfidfVectorizer(input='filename')\n",
    "\n",
    "\n",
    "    # Simple benchmark for number of features\n",
    "    result = []\n",
    "    for tfidf in [tfidf_stem, tfidf_word, tfidf_stop, tfidf_tokenizer_correct]:\n",
    "        tfidf.fit(paths_df.path.values)\n",
    "        result.append(len(tfidf.get_feature_names()))\n",
    "\n",
    "    result = pandas.DataFrame(result, columns=['len_of_features'], index=['tfidf_stem', 'tfidf_word', 'tfidf_stop', 'tfidf_tokenizer_correct'])\n",
    "    result = result.assign(difference=lambda x: (x.len_of_features - x.len_of_features.min()))\n",
    "    print(result)\n",
    "    pyplot.figure(1)\n",
    "    pyplot.bar([1,2,3,4], result.difference.values)\n",
    "    pyplot.xticks([1,2,3], result.index.values)\n",
    "    pyplot.ylabel('Number of tokens')\n",
    "    pyplot.xlabel('Method of tf-idf')\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to show number of tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Pipelines\n",
    "\n",
    "Método para criação de pipelines para tratamento dos dados em etapas utilizando os metodos de gaussian, bernoulli, multinomial, Linear e SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipes():\n",
    "    global pipes\n",
    "    t0 = time.time()\n",
    "    # Create pipes\n",
    "    pipes = {\n",
    "        'gaussianNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', GaussianNB())\n",
    "        ]),\n",
    "        'bernoulliNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', binary=True)),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', BernoulliNB())\n",
    "        ]),\n",
    "        'multinomialNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', MultinomialNB())\n",
    "        ]),\n",
    "        'linearSVC': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', LinearSVC())\n",
    "        ]),\n",
    "        'sgdclassifier': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', SGDClassifier(max_iter=5))\n",
    "        ]),\n",
    "    }\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to create pipes\")\n",
    "# Method to return params from pipe params adjusts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
