{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review ML\n",
    "\n",
    "## Importações \n",
    "\n",
    "### Bibliotecas para cálculos com array \n",
    "Pacotes de biblioteca cientificas para calculos com arrays n-dimensionais e outras funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos\n",
    "\n",
    "Modelos de classificação linear, Naive Bayes e SVN como soluções mais utilizadas nas pesquisas que são simples e que atingem o objetivo de lidar com textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engenharia de características\n",
    "\n",
    "Foram utilizadas as bibliotecas TextBlob e Tfidf, sendo essa propria da Scikit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dense_transform import DenseTransformer, tokenizer, tokenizer_correct\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,  CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos\n",
    "\n",
    "Para a geração de gráficos foi utilizado o Matplot lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação de Modelo\n",
    "\n",
    "Foi utilizada a biblioteca sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Processamento dos dados utilizando filtragem pelos metodos: gaussian, bernoulli, multinomial, Linear e SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time\n",
    "\n",
    "Biblioteca para acesso ao tempo e conversões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenção de Data\n",
    "\n",
    "Método utilizado para a obtenção dos dados que irão ser tratados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_data():\n",
    "    t0 = time.time()\n",
    "    global paths_df\n",
    "    # Load dirs name\n",
    "    cur_dir = os.path.realpath('.')\n",
    "    pos_dir = os.path.join(cur_dir, 'train-pos')\n",
    "    neg_dir = os.path.join(cur_dir, 'train-neg')\n",
    "\n",
    "\n",
    "    # Load files names\n",
    "    list_pos_dir = [ (os.path.join(pos_dir, x), 1) for x in os.listdir(pos_dir)][:11]\n",
    "    list_neg_dir = [ (os.path.join(neg_dir, x), 0) for x in os.listdir(neg_dir)][:11]\n",
    "    print(\"registers: {}\".format(len(list_pos_dir+list_neg_dir)))\n",
    "    print(\"Attention with 6000 registers it will consume about 5+GB of ram\")\n",
    "    # input(\"Continue? or press CTRL+C\")\n",
    "\n",
    "    # Mount data with label data frame\n",
    "    paths_df = pandas.DataFrame(list_pos_dir+list_neg_dir, columns=['path', 'label'])\n",
    "\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to obtain data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar números de tokens\n",
    "\n",
    "Metodo utilizado para a obtenção do número de tokens da aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_number_of_tokens():\n",
    "    t0 = time.time()\n",
    "    # Verify difference between size of tokens with tokenizer stem, stopwords\n",
    "    tfidf_stem = TfidfVectorizer(input='filename', stop_words='english', tokenizer=tokenizer)\n",
    "    tfidf_tokenizer_correct = TfidfVectorizer(input='filename', stop_words='english', tokenizer=tokenizer_correct)\n",
    "    tfidf_stem.fit(paths_df.path.values)\n",
    "    tfidf_stem.get_feature_names()\n",
    "    tfidf_stop = TfidfVectorizer(input='filename', stop_words='english')\n",
    "    tfidf_word = TfidfVectorizer(input='filename')\n",
    "\n",
    "\n",
    "    # Simple benchmark for number of features\n",
    "    result = []\n",
    "    for tfidf in [tfidf_stem, tfidf_word, tfidf_stop, tfidf_tokenizer_correct]:\n",
    "        tfidf.fit(paths_df.path.values)\n",
    "        result.append(len(tfidf.get_feature_names()))\n",
    "\n",
    "    result = pandas.DataFrame(result, columns=['len_of_features'], index=['tfidf_stem', 'tfidf_word', 'tfidf_stop', 'tfidf_tokenizer_correct'])\n",
    "    result = result.assign(difference=lambda x: (x.len_of_features - x.len_of_features.min()))\n",
    "    print(result)\n",
    "    pyplot.figure(1)\n",
    "    pyplot.bar([1,2,3,4], result.difference.values)\n",
    "    pyplot.xticks([1,2,3], result.index.values)\n",
    "    pyplot.ylabel('Number of tokens')\n",
    "    pyplot.xlabel('Method of tf-idf')\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to show number of tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Pipelines\n",
    "\n",
    "Método para criação de pipelines para tratamento dos dados em etapas utilizando os metodos de gaussian, bernoulli, multinomial, Linear e SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pipes():\n",
    "    global pipes\n",
    "    t0 = time.time()\n",
    "    # Create pipes\n",
    "    pipes = {\n",
    "        'gaussianNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', GaussianNB())\n",
    "        ]),\n",
    "        'bernoulliNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', binary=True)),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', BernoulliNB())\n",
    "        ]),\n",
    "        'multinomialNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', MultinomialNB())\n",
    "        ]),\n",
    "        'linearSVC': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', LinearSVC())\n",
    "        ]),\n",
    "        'sgdclassifier': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', SGDClassifier(max_iter=5))\n",
    "        ]),\n",
    "    }\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to create pipes\")\n",
    "# Method to return params from pipe params adjusts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de parâmetros\n",
    "\n",
    "Criação de um método que retorna parâmetros ajustados dos *pipes* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_params(best_params_):\n",
    "    return {'ngram_range': best_params_['vect__ngram_range'],\n",
    "      'use_idf': best_params_['vect__use_idf'],\n",
    "      'norm':  best_params_['vect__norm'],\n",
    "      'sublinear_tf': best_params_['vect__sublinear_tf'],\n",
    "      'stop_words': best_params_['vect__stop_words'],\n",
    "      'tokenizer': best_params_['vect__tokenizer']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição de parâmetros\n",
    "\n",
    "Método para definição de parâmetros para *pipeline* e verificação do tempo de processamento para definição destes parâmetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_params():\n",
    "    # Define params\n",
    "    t0 = time.time()\n",
    "    global parameters\n",
    "    parameters = {\n",
    "      'vect__ngram_range': [(1,1), (1,2)],\n",
    "      'vect__use_idf': (True, False),\n",
    "      'vect__norm':  ('l2', 'l1', None),\n",
    "      'vect__sublinear_tf': (True, False),\n",
    "      'vect__stop_words': ('english', None),\n",
    "      'vect__tokenizer': (None, tokenizer),\n",
    "    }\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to define parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento de dados no *pipeline*\n",
    "\n",
    "Métodos de processamento da *pipeline*, onde cada processo de filtragem é feito o treinamento em sequência para tratamento dos melhores valores dentro da cadeia de funções presentes no *pipeline*. Em cada um dos métodos é verificado o tempo de processamento para validação da performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gaussian_NB_pipeline():\n",
    "    t0 = time.time()\n",
    "    # Initialize best parameters search\n",
    "    parametrized = GridSearchCV(pipes['gaussianNB'], parameters, n_jobs=3)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedgaussianNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', GaussianNB())\n",
    "        ])\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to run gaussian NB pipeline\")\n",
    "\n",
    "def run_bernoulli_NB_pipeline():\n",
    "    t0 = time.time()\n",
    "    parametrized = GridSearchCV(pipes['bernoulliNB'], parameters, n_jobs=3)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedbernoulliNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', binary=True, **extract_params(parametrized.best_params_))),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', BernoulliNB())\n",
    "        ])\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to run bernoulli NB pipeline\")\n",
    "\n",
    "def run_multinomial_NB_pipeline():\n",
    "    t0 = time.time()\n",
    "\n",
    "    parametrized = GridSearchCV(pipes['multinomialNB'], parameters, n_jobs=3)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedmultinomialNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ( 'gnb', MultinomialNB())\n",
    "        ])\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to run multinomial NB pipeline\")\n",
    "\n",
    "def run_linearSVC_pipeline():\n",
    "    t0 = time.time()\n",
    "    parametrized = GridSearchCV(pipes['linearSVC'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedlinearSVC'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('gnb', LinearSVC())\n",
    "        ])\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to run linear SVC pipeline\")\n",
    "\n",
    "def run_sgdclassifier_pipeline():\n",
    "    t0 = time.time()\n",
    "\n",
    "    parametrized = GridSearchCV(pipes['linearSVC'], parameters, n_jobs=3)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedlinearSVC'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('gnb', LinearSVC())\n",
    "        ])\n",
    "    parametrized = GridSearchCV(pipes['sgdclassifier'], parameters, n_jobs=3)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedsgdclassifier'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('gnb', SGDClassifier())\n",
    "        ])\n",
    "\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to run SGD classifier pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento dos dados do *pipeline*\n",
    "\n",
    "Método para o treinamento dos dados tratados da *pipeline* utilizando a biblioteca pandas para as operações com *arrays* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mount_and_train():\n",
    "    t0 = time.time()\n",
    "    # Execute each pipe in dictionary pipes doing    # a score with test and train bases\n",
    "    # Variate the size of test and train bases\n",
    "    index = [x/10.0 for x in range(1,8)]\n",
    "    global df\n",
    "    df = pandas.DataFrame(index=index)\n",
    "    for pipe_name, pipe in pipes.items():\n",
    "        temp = []\n",
    "        for l in range(1,8):\n",
    "            # Split into train and test\n",
    "            # X - Train, Y - Train\n",
    "            # x - test, y - test\n",
    "            X, x, Y, y = train_test_split(\n",
    "                paths_df.path, paths_df.label, test_size=l/10.0, random_state=0\n",
    "            )\n",
    "            pipe.fit(X,Y)\n",
    "            temp.append([pipe.score(X,Y), pipe.score(x,y)])\n",
    "        columns = [\"train_{}\".format(pipe_name), \"test_{}\".format(pipe_name)]\n",
    "        new_df = pandas.DataFrame(temp, columns=columns, index=index)\n",
    "        df = df.join(new_df)\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to mount and train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos\n",
    "\n",
    "Demonstração de gráficos utilizando a biblioteca pyplot. Foi feita a geração dos dados obtidos em todas as etapas da *pipeline* junto com suas pontuações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_all():\n",
    "    t0 = time.time()\n",
    "    # Plot all\n",
    "    pyplot.figure(2)\n",
    "\n",
    "    pyplot.subplot(221)\n",
    "    pyplot.title('Gaussian NB')\n",
    "    pyplot.plot(df.train_gaussianNB)\n",
    "    pyplot.plot(df.test_gaussianNB)\n",
    "    pyplot.plot(df.test_optimizedgaussianNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "    pyplot.subplot(222)\n",
    "    pyplot.title('BernoulliNB')\n",
    "    pyplot.plot(df.train_bernoulliNB)\n",
    "    pyplot.plot(df.test_bernoulliNB)\n",
    "    pyplot.plot(df.test_optimizedbernoulliNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "\n",
    "    pyplot.subplot(223)\n",
    "    pyplot.title('MultinomialNB')\n",
    "    pyplot.plot(df.train_multinomialNB)\n",
    "    pyplot.plot(df.test_multinomialNB)\n",
    "    pyplot.plot(df.test_optimizedmultinomialNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "    pyplot.figure(3)\n",
    "    pyplot.subplot(222)\n",
    "    pyplot.title('LinearSVC')\n",
    "    pyplot.plot(df.train_linearSVC)\n",
    "    pyplot.plot(df.test_linearSVC)\n",
    "    pyplot.plot(df.test_optimizedlinearSVC, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "\n",
    "    pyplot.subplot(221)\n",
    "    pyplot.title('SGDClassifier')\n",
    "    pyplot.plot(df.train_sgdclassifier)\n",
    "    pyplot.plot(df.test_sgdclassifier)\n",
    "    pyplot.plot(df.test_optimizedsgdclassifier, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "    pyplot.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                        wspace=0.35)\n",
    "    print(df.ix[df.idxmax()])\n",
    "\n",
    "    pyplot.show()\n",
    "    print(\"It took: \" + str(time.time() - t0) + \" to plot graphs\")\n",
    "\n",
    "\n",
    "obtain_data()\n",
    "show_number_of_tokens()\n",
    "create_pipes()\n",
    "#extract_params()\n",
    "define_params()\n",
    "run_gaussian_NB_pipeline()\n",
    "run_bernoulli_NB_pipeline()\n",
    "run_multinomial_NB_pipeline()\n",
    "run_linearSVC_pipeline()\n",
    "run_sgdclassifier_pipeline()\n",
    "mount_and_train()\n",
    "plot_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
