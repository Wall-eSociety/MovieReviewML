{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review ML\n",
    "\n",
    "\n",
    "## Importações "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modelos\n",
    "\n",
    "Modelos de classificação linear, Naive Bayes e SVN como soluções mais utilizadas nas pesquisas que são simples e que atingem o objetivo de lidar com textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engenharia de características\n",
    "\n",
    "Foram utilizadas as bibliotecas TextBlob e Tfidf, sendo essa pŕopria da Scikit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dense_transform import DenseTransformer, tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,  CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos\n",
    "\n",
    "Para a geração de gráficos foi utilizado o Matplot lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenção de Dados\n",
    "Aqui são obtidos os dados a serem avaliados e carregamos eles para serem utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d823eef01bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Mount data with label data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpaths_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_pos_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist_neg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mobtain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-d823eef01bd5>\u001b[0m in \u001b[0;36mobtain_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobtain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Load dirs name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcur_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpos_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mneg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'neg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def obtain_data():\n",
    "    # Load dirs name\n",
    "    cur_dir = os.path.realpath('.')\n",
    "    pos_dir = os.path.join(cur_dir, 'pos')\n",
    "    neg_dir = os.path.join(cur_dir, 'neg')\n",
    "\n",
    "    # Load files names\n",
    "    list_pos_dir = [ (os.path.join(pos_dir, x), 1) for x in os.listdir(pos_dir)][:50]\n",
    "    list_neg_dir = [ (os.path.join(neg_dir, x), 0) for x in os.listdir(neg_dir)][:50]\n",
    "    print(\"registers: {}\".format(len(list_pos_dir+list_neg_dir)))\n",
    "    print(\"Attention with 6000 registers it will consume about 5+GB of ram\")\n",
    "    # input(\"Continue? or press CTRL+C\")\n",
    "    global paths_df\n",
    "    # Mount data with label data frame\n",
    "    paths_df = pandas.DataFrame(list_pos_dir+list_neg_dir, columns=['path', 'label'])\n",
    "obtain_data()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_number_of_tokens():\n",
    "\n",
    "    # Verify difference between size of tokens with tokenizer stem, stopwords\n",
    "    tfidf_stem = TfidfVectorizer(input='filename', stop_words='english', tokenizer=tokenizer)\n",
    "    tfidf_stop = TfidfVectorizer(input='filename', stop_words='english')\n",
    "    tfidf_word = TfidfVectorizer(input='filename')\n",
    "\n",
    "    # Simple benchmark for number of features\n",
    "    result = []\n",
    "    for tfidf in [tfidf_stem, tfidf_word, tfidf_stop]:\n",
    "        tfidf.fit(paths_df.path.values)\n",
    "        result.append(len(tfidf.get_feature_names()))\n",
    "\n",
    "    result = pandas.DataFrame(result, columns=['len_of_features'], index=['tfidf_stem', 'tfidf_word', 'tfidf_stop'])\n",
    "    result = result.assign(difference=lambda x: (x.len_of_features - x.len_of_features.min()))\n",
    "    print(result)\n",
    "    pyplot.figure(1)\n",
    "    pyplot.bar([1,2,3], result.difference.values)\n",
    "    pyplot.xticks([1,2,3], result.index.values)\n",
    "    pyplot.ylabel('Number of tokens')\n",
    "    pyplot.xlabel('Method of tf-idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Pipelines\n",
    "\n",
    "Aqui são criadas as pipelines para as buscas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_pipes():\n",
    "    global pipes\n",
    "    # Create pipes\n",
    "    pipes = {\n",
    "        'gaussianNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', GaussianNB())\n",
    "        ]),\n",
    "        'bernoulliNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', binary=True)),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', BernoulliNB())\n",
    "        ]),\n",
    "        'multinomialNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', MultinomialNB())\n",
    "        ]),\n",
    "        'linearSVC': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', LinearSVC())\n",
    "        ]),\n",
    "        'sgdclassifier': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', SGDClassifier(max_iter=5))\n",
    "        ]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_params(best_params_):\n",
    "    return {'ngram_range': best_params_['vect__ngram_range'],\n",
    "      'use_idf': best_params_['vect__use_idf'],\n",
    "      'norm':  best_params_['vect__norm'],\n",
    "      'sublinear_tf': best_params_['vect__sublinear_tf'],\n",
    "      'stop_words': best_params_['vect__stop_words'],\n",
    "      'tokenizer': best_params_['vect__tokenizer']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_params():\n",
    "    # Define params\n",
    "    global parameters\n",
    "    parameters = {\n",
    "      'vect__ngram_range': [(1,1), (1,2)],\n",
    "      'vect__use_idf': (True, False),\n",
    "      'vect__norm':  ('l2', 'l1', None),\n",
    "      'vect__sublinear_tf': (True, False),\n",
    "      'vect__stop_words': ('english', None),\n",
    "      'vect__tokenizer': (None, tokenizer),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gaussian_NB_pipeline():\n",
    "    # Initialize best parameters search\n",
    "    parametrized = GridSearchCV(pipes['gaussianNB'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path, paths_df.label)\n",
    "\n",
    "    pipes['optimizedgaussianNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', GaussianNB())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bernoulli_NB_pipeline():\n",
    "    parametrized = GridSearchCV(pipes['bernoulliNB'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedbernoulliNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', binary=True, **extract_params(parametrized.best_params_))),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', BernoulliNB())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multinomial_NB_pipeline():\n",
    "    parametrized = GridSearchCV(pipes['multinomialNB'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedmultinomialNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ( 'gnb', MultinomialNB())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linearSVC_pipeline():\n",
    "    parametrized = GridSearchCV(pipes['linearSVC'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedlinearSVC'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('gnb', LinearSVC())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sgdclassifier_pipeline():\n",
    "    parametrized = GridSearchCV(pipes['sgdclassifier'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedsgdclassifier'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('gnb', SGDClassifier())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mount_and_train():\n",
    "    # Execute each pipe in dictionary pipes doing\n",
    "    # a score with test and train bases\n",
    "    # Variate the size of test and train bases\n",
    "    index = [x/10.0 for x in range(1,8)]\n",
    "    global df\n",
    "    df = pandas.DataFrame(index=index)\n",
    "    for pipe_name, pipe in pipes.items():\n",
    "        temp = []\n",
    "        for l in range(1,8):\n",
    "            # Split into train and test\n",
    "            # X - Train, Y - Train\n",
    "            # x - test, y - test\n",
    "            X, x, Y, y = train_test_split(\n",
    "                paths_df.path, paths_df.label, test_size=l/10.0, random_state=0\n",
    "            )\n",
    "            pipe.fit(X,Y)\n",
    "            temp.append([pipe.score(X,Y), pipe.score(x,y)])\n",
    "        columns = [\"train_{}\".format(pipe_name), \"test_{}\".format(pipe_name)]\n",
    "        new_df = pandas.DataFrame(temp, columns=columns, index=index)\n",
    "        df = df.join(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotagem\n",
    "\n",
    "Aqui são plotados os gráficos das informações obtidas por meio do matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all():\n",
    "    # Plot all\n",
    "    pyplot.figure(2)\n",
    "\n",
    "    pyplot.subplot(221)\n",
    "    pyplot.title('Gaussian NB')\n",
    "    pyplot.plot(df.train_gaussianNB)\n",
    "    pyplot.plot(df.test_gaussianNB)\n",
    "    pyplot.plot(df.test_optimizedgaussianNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "    pyplot.subplot(222)\n",
    "    pyplot.title('BernoulliNB')\n",
    "    pyplot.plot(df.train_bernoulliNB)\n",
    "    pyplot.plot(df.test_bernoulliNB)\n",
    "    pyplot.plot(df.test_optimizedbernoulliNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "\n",
    "    pyplot.subplot(223)\n",
    "    pyplot.title('MultinomialNB')\n",
    "    pyplot.plot(df.train_multinomialNB)\n",
    "    pyplot.plot(df.test_multinomialNB)\n",
    "    pyplot.plot(df.test_optimizedmultinomialNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "    pyplot.figure(3)\n",
    "    pyplot.subplot(222)\n",
    "    pyplot.title('LinearSVC')\n",
    "    pyplot.plot(df.train_linearSVC)\n",
    "    pyplot.plot(df.test_linearSVC)\n",
    "    pyplot.plot(df.test_optimizedlinearSVC, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "\n",
    "    pyplot.subplot(221)\n",
    "    pyplot.title('SGDClassifier')\n",
    "    pyplot.plot(df.train_sgdclassifier)\n",
    "    pyplot.plot(df.test_sgdclassifier)\n",
    "    pyplot.plot(df.test_optimizedsgdclassifier, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "    pyplot.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                        wspace=0.35)\n",
    "    print(df.ix[df.idxmax()])\n",
    "\n",
    "    pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
