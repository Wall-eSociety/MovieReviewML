{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review ML\n",
    "\n",
    "\n",
    "## Importações "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modelos\n",
    "\n",
    "Modelos de classificação linear, Naive Bayes e SVN como soluções mais utilizadas nas pesquisas que são simples e que atingem o objetivo de lidar com textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engenharia de características\n",
    "\n",
    "Foram utilizadas as bibliotecas TexBlob e Tfidf, snedo essa pŕopria da Scikit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dense_transform import DenseTransformer, tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,  CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos\n",
    "\n",
    "Para a geração de gráficos foi utilizado o Matplot lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registers: 4\n",
      "Attention with 6000 registers it will consume about 5+GB of ram\n"
     ]
    }
   ],
   "source": [
    "def obtain_data():\n",
    "    # Load dirs name\n",
    "    cur_dir = os.path.realpath('.')\n",
    "    pos_dir = os.path.join(cur_dir, 'pos')\n",
    "    neg_dir = os.path.join(cur_dir, 'neg')\n",
    "\n",
    "    # Load files names\n",
    "    list_pos_dir = [ (os.path.join(pos_dir, x), 1) for x in os.listdir(pos_dir)][:50]\n",
    "    list_neg_dir = [ (os.path.join(neg_dir, x), 0) for x in os.listdir(neg_dir)][:50]\n",
    "    print(\"registers: {}\".format(len(list_pos_dir+list_neg_dir)))\n",
    "    print(\"Attention with 6000 registers it will consume about 5+GB of ram\")\n",
    "    # input(\"Continue? or press CTRL+C\")\n",
    "    global paths_df\n",
    "    # Mount data with label data frame\n",
    "    paths_df = pandas.DataFrame(list_pos_dir+list_neg_dir, columns=['path', 'label'])\n",
    "obtain_data()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_number_of_tokens():\n",
    "\n",
    "    # Verify difference between size of tokens with tokenizer stem, stopwords\n",
    "    tfidf_stem = TfidfVectorizer(input='filename', stop_words='english', tokenizer=tokenizer)\n",
    "    tfidf_stop = TfidfVectorizer(input='filename', stop_words='english')\n",
    "    tfidf_word = TfidfVectorizer(input='filename')\n",
    "\n",
    "    # Simple benchmark for number of features\n",
    "    result = []\n",
    "    for tfidf in [tfidf_stem, tfidf_word, tfidf_stop]:\n",
    "        tfidf.fit(paths_df.path.values)\n",
    "        result.append(len(tfidf.get_feature_names()))\n",
    "\n",
    "    result = pandas.DataFrame(result, columns=['len_of_features'], index=['tfidf_stem', 'tfidf_word', 'tfidf_stop'])\n",
    "    result = result.assign(difference=lambda x: (x.len_of_features - x.len_of_features.min()))\n",
    "    print(result)\n",
    "    pyplot.figure(1)\n",
    "    pyplot.bar([1,2,3], result.difference.values)\n",
    "    pyplot.xticks([1,2,3], result.index.values)\n",
    "    pyplot.ylabel('Number of tokens')\n",
    "    pyplot.xlabel('Method of tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_pipes():\n",
    "    global pipes\n",
    "    # Create pipes\n",
    "    pipes = {\n",
    "        'gaussianNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', GaussianNB())\n",
    "        ]),\n",
    "        'bernoulliNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', binary=True)),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', BernoulliNB())\n",
    "        ]),\n",
    "        'multinomialNB': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', MultinomialNB())\n",
    "        ]),\n",
    "        'linearSVC': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', LinearSVC())\n",
    "        ]),\n",
    "        'sgdclassifier': Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename')),\n",
    "          ( 'gnb', SGDClassifier(max_iter=5))\n",
    "        ]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_params(best_params_):\n",
    "    return {'ngram_range': best_params_['vect__ngram_range'],\n",
    "      'use_idf': best_params_['vect__use_idf'],\n",
    "      'norm':  best_params_['vect__norm'],\n",
    "      'sublinear_tf': best_params_['vect__sublinear_tf'],\n",
    "      'stop_words': best_params_['vect__stop_words'],\n",
    "      'tokenizer': best_params_['vect__tokenizer']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_params():\n",
    "    # Define params\n",
    "    global parameters\n",
    "    parameters = {\n",
    "      'vect__ngram_range': [(1,1), (1,2)],\n",
    "      'vect__use_idf': (True, False),\n",
    "      'vect__norm':  ('l2', 'l1', None),\n",
    "      'vect__sublinear_tf': (True, False),\n",
    "      'vect__stop_words': ('english', None),\n",
    "      'vect__tokenizer': (None, tokenizer),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gaussian_NB_pipeline():\n",
    "    # Initialize best parameters search\n",
    "    parametrized = GridSearchCV(pipes['gaussianNB'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path, paths_df.label)\n",
    "\n",
    "    pipes['optimizedgaussianNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', GaussianNB())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bernoulli_NB_pipeline():\n",
    "    parametrized = GridSearchCV(pipes['bernoulliNB'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedbernoulliNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', binary=True, **extract_params(parametrized.best_params_))),\n",
    "          ('dense', DenseTransformer()),\n",
    "          ('gnb', BernoulliNB())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multinomial_NB_pipeline():\n",
    "    parametrized = GridSearchCV(pipes['multinomialNB'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedmultinomialNB'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ( 'gnb', MultinomialNB())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linearSVC_pipeline():\n",
    "    parametrized = GridSearchCV(pipes['linearSVC'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedlinearSVC'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('gnb', LinearSVC())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sgdclassifier_pipeline():\n",
    "    parametrized = GridSearchCV(pipes['sgdclassifier'], parameters, n_jobs=1)\n",
    "    parametrized.fit(paths_df.path,paths_df.label)\n",
    "    print(parametrized.best_score_, parametrized.best_params_)\n",
    "\n",
    "    pipes['optimizedsgdclassifier'] = Pipeline([\n",
    "          ('vect', TfidfVectorizer(input='filename', **extract_params(parametrized.best_params_))),\n",
    "          ('gnb', SGDClassifier())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mount_and_train():\n",
    "    # Execute each pipe in dictionary pipes doing\n",
    "    # a score with test and train bases\n",
    "    # Variate the size of test and train bases\n",
    "    index = [x/10.0 for x in range(1,8)]\n",
    "    global df\n",
    "    df = pandas.DataFrame(index=index)\n",
    "    for pipe_name, pipe in pipes.items():\n",
    "        temp = []\n",
    "        for l in range(1,8):\n",
    "            # Split into train and test\n",
    "            # X - Train, Y - Train\n",
    "            # x - test, y - test\n",
    "            X, x, Y, y = train_test_split(\n",
    "                paths_df.path, paths_df.label, test_size=l/10.0, random_state=0\n",
    "            )\n",
    "            pipe.fit(X,Y)\n",
    "            temp.append([pipe.score(X,Y), pipe.score(x,y)])\n",
    "        columns = [\"train_{}\".format(pipe_name), \"test_{}\".format(pipe_name)]\n",
    "        new_df = pandas.DataFrame(temp, columns=columns, index=index)\n",
    "        df = df.join(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all():\n",
    "    # Plot all\n",
    "    pyplot.figure(2)\n",
    "\n",
    "    pyplot.subplot(221)\n",
    "    pyplot.title('Gaussian NB')\n",
    "    pyplot.plot(df.train_gaussianNB)\n",
    "    pyplot.plot(df.test_gaussianNB)\n",
    "    pyplot.plot(df.test_optimizedgaussianNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "    pyplot.subplot(222)\n",
    "    pyplot.title('BernoulliNB')\n",
    "    pyplot.plot(df.train_bernoulliNB)\n",
    "    pyplot.plot(df.test_bernoulliNB)\n",
    "    pyplot.plot(df.test_optimizedbernoulliNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "\n",
    "    pyplot.subplot(223)\n",
    "    pyplot.title('MultinomialNB')\n",
    "    pyplot.plot(df.train_multinomialNB)\n",
    "    pyplot.plot(df.test_multinomialNB)\n",
    "    pyplot.plot(df.test_optimizedmultinomialNB, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "    pyplot.figure(3)\n",
    "    pyplot.subplot(222)\n",
    "    pyplot.title('LinearSVC')\n",
    "    pyplot.plot(df.train_linearSVC)\n",
    "    pyplot.plot(df.test_linearSVC)\n",
    "    pyplot.plot(df.test_optimizedlinearSVC, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "\n",
    "\n",
    "    pyplot.subplot(221)\n",
    "    pyplot.title('SGDClassifier')\n",
    "    pyplot.plot(df.train_sgdclassifier)\n",
    "    pyplot.plot(df.test_sgdclassifier)\n",
    "    pyplot.plot(df.test_optimizedsgdclassifier, 'r--')\n",
    "    pyplot.ylabel(\"Score\")\n",
    "    pyplot.xlabel(\"% size train base\")\n",
    "    pyplot.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                        wspace=0.35)\n",
    "    print(df.ix[df.idxmax()])\n",
    "\n",
    "    pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
